# Beter Woorden

Application for Dutch practice by filling in the missing word. Uses an LLM to extract
practice sentences from NOS articles, chooses a word and makes you guess which word goes
where.

This was created for my own purposes for language study, however it also serves as a good
exercise in building a full stack, fully serverless LLM-based app using LangGraph,
Next.js, and Supabase.

Uses AWS Lambda to support making longer LLM calls, which is sometimes necessary when
invoking free models. The Supabase Edge Function runs the LangGraph agent which grabs a
random NOS article and submits a job to the Lambda worker that generates the quiz via a
free model on OpenRouter with structured decoding.

The Next.js/Supabase application is in the `app` directory.  It can be deployed by linking
to a Supabase project and deploying the database and functions.

The AWS Lambda is in the `lambda` directory.  It can be deployed by SAM using
`lambda/deploy.sh`.

The frontend is built and deployed to Github Pages using a Github Actions workflow. It
does not currently auto-deploy the Supabase or AWS Lambda parts, just the static SPA
files.

You should create files `app/.env.local` and `app/.env.production` defining:

- `AWS_LAMBDA_ENDPOINT` (optional, for local testing against `sam local start-lambda`)
- `AWS_REGION`
- `AWS_ACCESS_KEY_ID` (generated during lambda deploy)
- `AWS_SECRET_ACCESS_KEY` (generated during lambda deploy)
- `GENERATE_QUIZ_WORKER_LAMBDA=QuizWorkerFunction`
- `NEXT_PUBLIC_SUPABASE_URL` (generated by supabase project, or `https://localhost:1234/v1`)
- `NEXT_PUBLIC_SUPABASE_ANON_KEY` (generated by supabase project, publishable key)

You should also create `lambda/env.local.json` and `lambda/env.prod.json` defining:

```
{
  "QuizWorkerFunction": {
    "LLM_API_KEY": "<openrouter key>",
    "LLM_BASE_URL": "https://openrouter.ai/api/v1",
    "LLM_MODEL": "xiaomi/mimo-v2-flash:free"
  }
}
```

For local testing I have used LM Studio on some small models when running the worker as a
server, however I had trouble getting SAM's docker container to break out and correctly
call a localhost server, so YMMV.

The use of an external Lambda is not strictly necessary and could be done from the
Supabase Edge Function directly, however I found that some free model calls on OpenRouter
can be *very* slow, several minutes, and therefore I needed something that would allow me
to set the timeout to several minutes. At first I used a t2.micro instance, but my goal
was to get this fully serverless, and Lambda supports up to 15 minutes which should be
sufficient for any real use case. But if we used paid calls here, it would probably be
unnecessary.

On model selection: most 30B+ models can handle this task just fine, currently I am using
Mimo which seems to do very well but occasionally generates bad questions. This is par for
the course, but some improvements to the agent may work around this.  I found that current
Gemini models do a perfect job, but I stuck to free models here.


2026 Stephen Sinclair <radarsat1@gmail.com>, see LICENSE.

Disclaimer: a mostly "careful-vibe-coded" project with Gemini.
